# -----------------------------------------------------------------------
# Copyright (c) 2021 SAP SE or an SAP affiliate company. All rights reserved.
# -----------------------------------------------------------------------

# Specifies the location of the spring context file putted automatically to the global platform application context.
processing.application-context=processing-spring.xml,cronjob-spring.xml,task-spring.xml,processengine-spring.xml,processing-jmx-spring.xml,processing-distributed-spring.xml

processing.tomcat.tld.scan=javax.servlet.jsp.jstl-*.jar
processing.tomcat.tld.default.scan.enabled=false
processing.tomcat.pluggability.scan=javax.servlet.jsp.jstl-*.jar
processing.tomcat.pluggability.default.scan.enabled=false

# -----------------------------------------------------------------------
# Node configuration
# -----------------------------------------------------------------------

# Allows to exclude a set of cluster nodes from running the task engine.
# Even without enabled engine it is possible to schedule new tasks on such
# nodes. The only difference is that they must be performed by other nodes. 

#task.excluded.cluster.ids=1,4,12

# -----------------------------------------------------------------------
# Worker configuration
# -----------------------------------------------------------------------

# The maximum number of workers to be used for performing tasks. 
task.workers.max=10

# The time to keep a idle worker alive before removing it from pool.
task.workers.idle=20

# -----------------------------------------------------------------------
# Polling configuration
# -----------------------------------------------------------------------

# The time between two attempts for getting new tasks to perform.
task.polling.interval=10

#the amount of time (in seconds) waiting for completion of all tasks during shutdown process (see DefaultTaskService.destroy()) 
tasks.shutdown.wait=15

#enables task processing, 'true' by default, setting it to 'false' disables it (tasks and cronjobs will not be executed on the current node)
task.engine.loadonstartup=true

#allows to specify how many tasks are queried. #workers variable can be used to get number of current worker threads 
task.engine.query.tasks.count=#workers * 200

#allows to specify how many conditions are queried. #workers variable can be used to get number of current worker threads
task.engine.query.conditions.count=#workers * 200

# defines the time threshold differentiating the current tasks and old tasks.
# If greater than 0 and task.engine.query.full.interval property is set, the old tasks will be checked less often than current tasks.
# If value <= 0 then all tasks will be checked whether they can be processed
#
# Value in number of hours.
task.engine.query.full.executiontime.threshold=0

# defines the interval of retrieving old and expired tasks to process. If greater than 0, the old and expired tasks
# will be checked every n-th poll, where n is the value of this property. If equal to 0, old and expired tasks will be checked
# with every poll
task.engine.query.full.interval=0

# allows to specify (in seconds) the minimal interval for polling the database for new tasks;
# if the polling is executed more frequently, the tasks buffered from last poll will be used until the interval is assured
task.polling.interval.min=10

# allows you to specify (in seconds) the interval of count queries executed to report statistics of tasks to be executed
# default value is -1 which means no count queries is executed
# value 0 means that query will be executed with the polling query  
task.queue.size.reporting.interval=60

# If set to true forces task engine to process only task assigned to the same node on which task engine is running.
# Default value is false
task.engine.exclusive.mode=false

# Set maximum numer of attempts when trying to recover from taskengine startup errors
task.engine.retry.maxattempts=5 

# Set the backoff period for retry in milliseconds
task.engine.retry.backoffperiod=2000

# defines the interval (in seconds) at which each node should update (or insert if not existing) it's heartbeat in auxiliary
# table. The heartbeat is used to determine whether the node is still online and working
task.auxiliaryTables.worker.registration.interval.seconds=5

# defines the interval (in seconds) how often the scheduler should try to fill the tasks' queue
task.auxiliaryTables.scheduler.interval.seconds=2 * ${task.auxiliaryTables.worker.registration.interval.seconds}

# defines the threshold (in seconds) of tasks' waiting time in queue. The tasks that has not been processed within this time
# will be removed from queue (if the task is still present in tasks table, it will be rescheduled next time)
task.auxiliaryTables.scheduler.cleanQueue.oldTasksThreshold.seconds=100 * ${task.auxiliaryTables.scheduler.interval.seconds}

# defines the interval since current time in which the node's heartbeat is treated as valid
task.auxiliaryTables.worker.activation.interval.seconds=4 * ${task.auxiliaryTables.worker.registration.interval.seconds}

# defines the interval after which the node will be deactivated. If node doesn't update the heartbeat during this interval
# it will not be taken into account when scheduling takes place
task.auxiliaryTables.worker.deactivation.interval.seconds=2 * ${task.auxiliaryTables.worker.activation.interval.seconds}

# defines the interval after which the node will be removed. If node # doesn't update it's heartbeat during this interval
# all tasks assigned to this node will be unassigned
task.auxiliaryTables.worker.removal.interval.seconds=10 * ${task.auxiliaryTables.worker.activation.interval.seconds}

# the multiplier of number of tasks that should be available in the queue for each node. The base value is equal
# tasks.workers.max * 2 and then multiplied by the value of this property. If the number of tasks available in queue for any
# node is less then the calculated amount, then the scheduler will try to move all valid tasks to queue
task.auxiliaryTables.worker.lowTasksCountThreshold.multiplier=20

# the multiplier of number of maximum tasks which might be polled from database in one worker cycle.
# The base value is equal to runningState.getMaxThreads() * 2  and then multiplied by the value of this property.
task.auxiliaryTables.worker.tasks.count.multiplier=2

# max number of tasks that should be batch-removed from queue
task.auxiliaryTables.worker.deleteTasks.maxBatchSize=100

# the lowest value of range (inclusive) assigned to tasks in queue
task.auxiliaryTables.tasks.range.start=0

# the highest value of range (inclusive) assigned to tasks in queue
task.auxiliaryTables.tasks.range.end=1000

# number of seconds that task in queue should be locked for processing by other nodes
task.auxiliaryTables.tasks.lockDuration.seconds=${task.auxiliaryTables.worker.deactivation.interval.seconds}

# multiplier for the buffered version of auxiliary tables based tasks provider. The property defines the multiplier
# for the number of tasks that should be obtained from the db. Those tasks are then buffered in memory and returned
# to task engine in parts. The buffer is going to hold all tasks until it is empty. The repoll will occur only if
# buffer has not enough tasks AND the minimal interval since last poll has been reached (defined by task.polling.interval.min)
task.auxiliaryTables.buffer.multiplier=10

# the property defines the tasks provider that should be used by task engine to poll tasks.
# The values allowed are defined by beans declared in spring configuration that implements
# the de.hybris.platform.task.impl.TasksProvider interface. If the key provided is not valid the tasks provider
# defined as default will be used
task.polling.provider=

# If set to true forces task engine to postpone polling the database for tasks until all servlet contexts are started
# Default value is false
task.polling.startup.delay.enabled=false

#processengine
processengine.process.log.pattern=%-5p %X{RemoteAddr}%X{Tenant}[%c{1}] %m%n

# If set to true, process engine will set attribute canjoinpreviousnode for action node/script node to true 
# if this attribute isn't present.
# Default value is true
processengine.process.canjoinpreviousnode.default=true

# If set to true, task engine tasks' logger will persist the log file in database. The log file should contain logs
# gathered while processing the task by task engine
# Default value is true
processengine.process.log.dbstore.enabled=true

# Restart process request
# Number of retires and milliseconds between next retries when requesting a business process restart
processengine.process.restart.retries=3
processengine.process.restart.millis=500
# Should there be an exception if request for business process restart fails to restart that process
processengine.process.restart.exception.if.failed=true

# Turn back old behaviour of business process restart (no validation of current process run, explicit restart)
# Warning: legacy behaviour can cause a corrupt business process due to concurrent execution after restart
processengine.process.restart.legacy=false

#Enables marking task as done to make business process execution more resilient
mark.process.as.done.enabled=false

# -----------------------------------------------------------------------
# Distributed Processes internal settings
# -----------------------------------------------------------------------
#
# Since distributed processes run fully asynchronous there can be a small delay between 
# status changes become visible in both process and task item (due to cache invalidation 
# being iterative). To mitigate a wait delay can be defined in milliseconds:    
#
distributed.processes.transitions.stale.status.max.wait=10000

#
# Switching distributed process status may require several attempts due to its asynchronous 
# nature. This setting defines the maximum allowed number of retries.
#
distributed.processes.transitions.max.retries=5

# -----------------------------------------------------------------------
# Simple Template for Distributed Processes settings
# -----------------------------------------------------------------------

distributed.process.simple.template.max.batch.retries=3
distributed.process.simple.template.batch.size=100

keygen.distributed.process.code.name=ext.processing
keygen.distributed.process.code.digits=8
keygen.distributed.process.code.start=00000000
keygen.distributed.process.code.type=alphanumeric
keygen.distributed.process.code.template=$


# secure media folder
media.folder.cronjob.secured=true

# Trigger task runner settings
# set maximum number of attempts when unexpected exception happen during execution of job
taskRunner.trigger.retry.maxattempts=10
# Defines the interval in seconds after which failed job will be retried. 
# It only applies if next time job execution has not been calculated
taskRunner.trigger.retry.interval.seconds=300


# If set to true, there will be a thread active that will try to unlock the cronjobs running on cluster nodes, that have
# already been shutdown. The thread will be started only if clustering and autodiscovery is enabled
cronjob.unlocker.active=${cluster.nodes.autodiscovery}
# The interval (in ms) defining how often the thread should try to unlock the cronjobs. Minimal valid value is 1000 - in
# case of lower value provided 1000ms will be used
# Default value is equal to 5min (5x60x1000)
cronjob.unlocker.interval.ms=300000
# The thread is looking for nodes that are still registered in 'CLNodeInfos' table but has not updated it's ping for some time
# This property defines how long should node be inactive to be treated as stale
# Default value is equal to 30min (30x60x1000)
cronjob.unlocker.stale.node.interval.ms=1800000
# This property allows to set the cutoff for the stale node lookup. The thread will ignore the nodes that are inactive
# for a longer period than 'cronjob.unlocker.stale.node.interval.ms' minus value of this property. If set to 0 or less,
# thread will not ignore any nodes (will look for nodes with last ping since EPOCH time).
cronjob.unlocker.stale.node.cutoff.interval.seconds=1800
# This property defines how many cronjobs (at maximum) should be unlocked during each pass of unlocking thread.
# If set to 0 or less, there will be no limit - all cronjobs "running" on stale nodes should be unlocked as soon as possible
cronjob.unlocker.cronJobs.unlockLimit=200

#This property defines times threshold (number of hours) after which cron job history entry can be reset to ABORTED status
cronjob.history.reset.threshold.hours=24
